{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-833f22c582ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import configparser\n",
    "import csv\n",
    "import os\n",
    "import os.path as osp\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import scipy\n",
    "import torch\n",
    "\n",
    "\n",
    "class MOT17ObjDetect(torch.utils.data.Dataset):\n",
    "    \"\"\" Data class for the Multiple Object Tracking Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root, transforms=None, vis_threshold=0.25):\n",
    "        self.root = root  #root = dataset path? \"gdrive/MyDrive/Colab_Notebooks/faster_rcnn_fpn_training_mot_17/train\"\n",
    "        self.transforms = transforms\n",
    "        self._vis_threshold = vis_threshold\n",
    "        self._classes = ('background', 'pedestrian') #classes for prediction\n",
    "        self._img_paths = []  # images path\n",
    "\n",
    "        for f in os.listdir(root):  # for every f i.e., MOT17-02,MOT17-04, etc present in train or test dataset folder\n",
    "            path = os.path.join(root, f) #joining train + MOT17-02\n",
    "            config_file = os.path.join(path, 'seqinfo.ini')  # seqinfo.ini has information about the dataset....\n",
    "\n",
    "            assert os.path.exists(config_file), \\\n",
    "                'Path does not exist: {}'.format(config_file) # checking if path exists\n",
    "\n",
    "            config = configparser.ConfigParser()\n",
    "            config.read(config_file) #reading config file i.e., seqinfo.ini\n",
    "            seq_len = int(config['Sequence']['seqLength'])  #fetching seqlength\n",
    "            im_width = int(config['Sequence']['imWidth'])  #fetching image width\n",
    "            im_height = int(config['Sequence']['imHeight'])  #fetching image height\n",
    "            im_ext = config['Sequence']['imExt']  #fetching file ext i.e., jpg or png\n",
    "            im_dir = config['Sequence']['imDir']  #image directory i.e., img1\n",
    "\n",
    "            _imDir = os.path.join(path, im_dir)  # joining image dir path i.e., gdrive/MyDrive/Colab_Notebooks/faster_rcnn_fpn_training_mot_17/train/MOT17-02/img1\n",
    "\n",
    "            for i in range(1, seq_len + 1):\n",
    "                img_path = os.path.join(_imDir, f\"{i:06d}{im_ext}\")  # 6 0's + i and .jpg \n",
    "                assert os.path.exists(img_path), \\\n",
    "                    'Path does not exist: {img_path}'  #checking if path exists\n",
    "                # self._img_paths.append((img_path, im_width, im_height))\n",
    "                self._img_paths.append(img_path)  #appending img path so _img_paths = [\"train/MOT17-02/img1/000001.jpg,...\"]\n",
    "\n",
    "    @property\n",
    "    def num_classes(self):\n",
    "        return len(self._classes)\n",
    "\n",
    "    def _get_annotation(self, idx):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "\n",
    "        if 'test' in self.root:\n",
    "          \n",
    "            num_objs = 0\n",
    "            boxes = torch.zeros((num_objs, 4), dtype=torch.float32)\n",
    "\n",
    "            return {'boxes': boxes,\n",
    "                'labels': torch.ones((num_objs,), dtype=torch.int64),\n",
    "                'image_id': torch.tensor([idx]),\n",
    "                'area': (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]),\n",
    "                'iscrowd': torch.zeros((num_objs,), dtype=torch.int64),\n",
    "                'visibilities': torch.zeros((num_objs), dtype=torch.float32)}\n",
    "                \n",
    "        img_path = self._img_paths[idx]  #idx = image index which is given through param\n",
    "        file_index = int(os.path.basename(img_path).split('.')[0])\n",
    "\n",
    "        gt_file = os.path.join(os.path.dirname(\n",
    "            os.path.dirname(img_path)), 'gt', 'gt.txt')  # ground truth\n",
    "\n",
    "        assert os.path.exists(gt_file), \\\n",
    "            'GT file does not exist: {}'.format(gt_file)\n",
    "\n",
    "        bounding_boxes = []\n",
    "\n",
    "        with open(gt_file, \"r\") as inf:\n",
    "            reader = csv.reader(inf, delimiter=',')\n",
    "            for row in reader:\n",
    "                visibility = float(row[8])\n",
    "                if int(row[0]) == file_index and int(row[6]) == 1 and int(row[7]) == 1 and visibility >= self._vis_threshold:\n",
    "                    bb = {}\n",
    "                    bb['bb_left'] = int(row[2])  #912\n",
    "                    bb['bb_top'] = int(row[3])    #484\n",
    "                    bb['bb_width'] = int(row[4])  #97\n",
    "                    bb['bb_height'] = int(row[5]) #109\n",
    "                    bb['visibility'] = float(row[8])  #1\n",
    "\n",
    "                    bounding_boxes.append(bb)\n",
    "\n",
    "        num_objs = len(bounding_boxes)\n",
    "\n",
    "        boxes = torch.zeros((num_objs, 4), dtype=torch.float32)\n",
    "        visibilities = torch.zeros((num_objs), dtype=torch.float32)\n",
    "        \n",
    "        for i, bb in enumerate(bounding_boxes):\n",
    "            # Make pixel indexes 0-based, should already be 0-based (or not)\n",
    "            x1 = bb['bb_left'] - 1\n",
    "            y1 = bb['bb_top'] - 1\n",
    "            # This -1 accounts for the width (width of 1 x1=x2)\n",
    "            x2 = x1 + bb['bb_width'] - 1\n",
    "            y2 = y1 + bb['bb_height'] - 1\n",
    "\n",
    "            boxes[i, 0] = x1\n",
    "            boxes[i, 1] = y1\n",
    "            boxes[i, 2] = x2\n",
    "            boxes[i, 3] = y2\n",
    "            visibilities[i] = bb['visibility']\n",
    "            \n",
    "        return {'boxes': boxes,\n",
    "                'labels': torch.ones((num_objs,), dtype=torch.int64),\n",
    "                'image_id': torch.tensor([idx]),\n",
    "                'area': (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]),\n",
    "                'iscrowd': torch.zeros((num_objs,), dtype=torch.int64),\n",
    "                'visibilities': visibilities,}\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images ad masks\n",
    "        img_path = self._img_paths[idx]\n",
    "        # mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        target = self._get_annotation(idx)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._img_paths)\n",
    "    \n",
    "    def write_results_files(self, results, output_dir):\n",
    "        \"\"\"Write the detections in the format for MOT17Det sumbission\n",
    "\n",
    "        all_boxes[image] = N x 5 array of detections in (x1, y1, x2, y2, score)\n",
    "\n",
    "        Each file contains these lines:\n",
    "        <frame>, <id>, <bb_left>, <bb_top>, <bb_width>, <bb_height>, <conf>, <x>, <y>, <z>\n",
    "\n",
    "        Files to sumbit:\n",
    "        ./MOT17-01.txt\n",
    "        ./MOT17-02.txt\n",
    "        ./MOT17-03.txt\n",
    "        ./MOT17-04.txt\n",
    "        ./MOT17-05.txt\n",
    "        ./MOT17-06.txt\n",
    "        ./MOT17-07.txt\n",
    "        ./MOT17-08.txt\n",
    "        ./MOT17-09.txt\n",
    "        ./MOT17-10.txt\n",
    "        ./MOT17-11.txt\n",
    "        ./MOT17-12.txt\n",
    "        ./MOT17-13.txt\n",
    "        ./MOT17-14.txt\n",
    "        \"\"\"\n",
    "\n",
    "        #format_str = \"{}, -1, {}, {}, {}, {}, {}, -1, -1, -1\"\n",
    "\n",
    "        files = {}\n",
    "        for image_id, res in results.items():\n",
    "            path = self._img_paths[image_id]\n",
    "            img1, name = osp.split(path)\n",
    "            # get image number out of name\n",
    "            frame = int(name.split('.')[0])\n",
    "            # smth like /train/MOT17-09-FRCNN or /train/MOT17-09\n",
    "            tmp = osp.dirname(img1)\n",
    "            # get the folder name of the sequence and split it\n",
    "            tmp = osp.basename(tmp).split('-')\n",
    "            # Now get the output name of the file\n",
    "            out = tmp[0]+'-'+tmp[1]+'.txt'\n",
    "            outfile = osp.join(output_dir, out)\n",
    "\n",
    "            # check if out in keys and create empty list if not\n",
    "            if outfile not in files.keys():\n",
    "                files[outfile] = []\n",
    "\n",
    "            for box, score in zip(res['boxes'], res['scores']):\n",
    "                x1 = box[0].item()\n",
    "                y1 = box[1].item()\n",
    "                x2 = box[2].item()\n",
    "                y2 = box[3].item()\n",
    "                files[outfile].append(\n",
    "                    [frame, -1, x1, y1, x2 - x1, y2 - y1, score.item(), -1, -1, -1])\n",
    "\n",
    "        for k, v in files.items():\n",
    "            with open(k, \"w\") as of:\n",
    "                writer = csv.writer(of, delimiter=',')\n",
    "                for d in v:\n",
    "                    writer.writerow(d)\n",
    "\n",
    "    def print_eval(self, results, ovthresh=0.5):\n",
    "        \"\"\"Evaluates the detections (not official!!)\n",
    "\n",
    "        all_boxes[cls][image] = N x 5 array of detections in (x1, y1, x2, y2, score)\n",
    "        \"\"\"\n",
    "\n",
    "        if 'test' in self.root:\n",
    "            print('No GT data available for evaluation.')\n",
    "            return\n",
    "            \n",
    "        # Lists for tp and fp in the format tp[cls][image]\n",
    "        tp = [[] for _ in range(len(self._img_paths))]\n",
    "        fp = [[] for _ in range(len(self._img_paths))]\n",
    "\n",
    "        npos = 0\n",
    "        gt = []\n",
    "        gt_found = []\n",
    "\n",
    "        for idx in range(len(self._img_paths)):\n",
    "            annotation = self._get_annotation(idx)\n",
    "            bbox = annotation['boxes'][annotation['visibilities'].gt(self._vis_threshold)]\n",
    "            found = np.zeros(bbox.shape[0])\n",
    "            gt.append(bbox.cpu().numpy())\n",
    "            gt_found.append(found)\n",
    "\n",
    "            npos += found.shape[0]\n",
    "\n",
    "        # Loop through all images\n",
    "        # for res in results:\n",
    "        for im_index, (im_gt, found) in enumerate(zip(gt, gt_found)):\n",
    "            # Loop through dets an mark TPs and FPs\n",
    "            \n",
    "            # im_index = res['image_id'].item()\n",
    "            # im_det = results['boxes']\n",
    "            # annotation = self._get_annotation(im_index)\n",
    "            # im_gt = annotation['boxes'][annotation['visibilities'].gt(0.5)].cpu().numpy()\n",
    "            # found = np.zeros(im_gt.shape[0])\n",
    "            \n",
    "            im_det = results[im_index]['boxes'].cpu().numpy()\n",
    "\n",
    "            im_tp = np.zeros(len(im_det))\n",
    "            im_fp = np.zeros(len(im_det))\n",
    "            for i, d in enumerate(im_det):\n",
    "                ovmax = -np.inf\n",
    "\n",
    "                if im_gt.size > 0:\n",
    "                    # compute overlaps\n",
    "                    # intersection\n",
    "                    ixmin = np.maximum(im_gt[:, 0], d[0])\n",
    "                    iymin = np.maximum(im_gt[:, 1], d[1])\n",
    "                    ixmax = np.minimum(im_gt[:, 2], d[2])\n",
    "                    iymax = np.minimum(im_gt[:, 3], d[3])\n",
    "                    iw = np.maximum(ixmax - ixmin + 1., 0.)\n",
    "                    ih = np.maximum(iymax - iymin + 1., 0.)\n",
    "                    inters = iw * ih\n",
    "\n",
    "                    # union\n",
    "                    uni = ((d[2] - d[0] + 1.) * (d[3] - d[1] + 1.) +\n",
    "                            (im_gt[:, 2] - im_gt[:, 0] + 1.) *\n",
    "                            (im_gt[:, 3] - im_gt[:, 1] + 1.) - inters)\n",
    "\n",
    "                    overlaps = inters / uni\n",
    "                    ovmax = np.max(overlaps)\n",
    "                    jmax = np.argmax(overlaps)\n",
    "\n",
    "                if ovmax > ovthresh:\n",
    "                    if found[jmax] == 0:\n",
    "                        im_tp[i] = 1.\n",
    "                        found[jmax] = 1.\n",
    "                    else:\n",
    "                        im_fp[i] = 1.\n",
    "                else:\n",
    "                    im_fp[i] = 1.\n",
    "\n",
    "            tp[im_index] = im_tp\n",
    "            fp[im_index] = im_fp\n",
    "\n",
    "        # Flatten out tp and fp into a numpy array\n",
    "        i = 0\n",
    "        for im in tp:\n",
    "            if type(im) != type([]):\n",
    "                i += im.shape[0]\n",
    "\n",
    "        tp_flat = np.zeros(i)\n",
    "        fp_flat = np.zeros(i)\n",
    "\n",
    "        i = 0\n",
    "        for tp_im, fp_im in zip(tp, fp):\n",
    "            if type(tp_im) != type([]):\n",
    "                s = tp_im.shape[0]\n",
    "                tp_flat[i:s+i] = tp_im\n",
    "                fp_flat[i:s+i] = fp_im\n",
    "                i += s\n",
    "\n",
    "        tp = np.cumsum(tp_flat)\n",
    "        fp = np.cumsum(fp_flat)\n",
    "        rec = tp / float(npos)\n",
    "        # avoid divide by zero in case the first detection matches a difficult\n",
    "        # ground truth (probably not needed in my code but doesn't harm if left)\n",
    "        prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n",
    "        tmp = np.maximum(tp + fp, np.finfo(np.float64).eps)\n",
    "\n",
    "        # correct AP calculation\n",
    "        # first append sentinel values at the end\n",
    "        mrec = np.concatenate(([0.], rec, [1.]))\n",
    "        mpre = np.concatenate(([0.], prec, [0.]))\n",
    "\n",
    "        # compute the precision envelope\n",
    "        for i in range(mpre.size - 1, 0, -1):\n",
    "            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
    "\n",
    "        # to calculate area under PR curve, look for points\n",
    "        # where X axis (recall) changes value\n",
    "        i = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "\n",
    "        # and sum (\\Delta recall) * prec\n",
    "        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
    "\n",
    "        tp, fp, prec, rec, ap = np.max(tp), np.max(fp), prec[-1], np.max(rec), ap\n",
    "        \n",
    "        print(f\"AP: {ap} Prec: {prec} Rec: {rec} TP: {tp} FP: {fp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#import transforms as T\n",
    "\n",
    "#dataset = MOT17ObjDetect('gdrive/MyDrive/Colab_Notebooks/faster_rcnn_fpn_training_mot_17/train')\n",
    "#img, target = dataset[200]\n",
    "\n",
    "def plot(img, boxes, path):\n",
    "  fig, ax = plt.subplots(1, dpi=96)\n",
    "\n",
    "  img = img.mul(255).permute(1, 2, 0).byte().numpy()\n",
    "  width, height, _ = img.shape\n",
    "    \n",
    "  ax.imshow(img, cmap='gray')   # image\n",
    "  fig.set_size_inches(width / 80, height / 80)  # boundary of image\n",
    "\n",
    "  for box in boxes:\n",
    "      rect = plt.Rectangle(\n",
    "        (box[0], box[1]),\n",
    "        box[2] - box[0],\n",
    "        box[3] - box[1],\n",
    "        fill=False,\n",
    "        linewidth=1.0)\n",
    "      ax.add_patch(rect)  \n",
    "\n",
    "  plt.axis('off')\n",
    "  plt.show()\n",
    "  fig.savefig(path)\n",
    "\n",
    "#checking if plot() works\n",
    "#img, target = T.ToTensor()(img, target)\n",
    "#plot(img, target['boxes'], \"sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# get detection model based on FasterRCNN with FPN and ResNet50\n",
    "def get_detection_model(num_classes):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    model.roi_heads.nms_thresh = 0.3\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "import transforms as T\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    # converts the image, a PIL image, into a PyTorch Tensor\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        # during training, randomly flip the training images and ground-truth for data augmentation\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "#dataset\n",
    "dataset = MOT17ObjDetect(dataset_path, get_transform(train=False))  # dataset path is fetched from above\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_detection_model(dataset.num_classes)\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# load dataset with dataloader\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=1, shuffle=False, num_workers=2,  # False, 4\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "#load saved model\n",
    "model.load_state_dict(torch.load(\"gdrive/MyDrive/Colab_Notebooks/faster_rcnn_fpn_training_mot_17/model_epoch_27.model\"))\n",
    "\n",
    "# put the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "print(\"Model Loaded Successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = 'gdrive/MyDrive/Colab_Notebooks/out/' #@param ['gdrive/MyDrive/Colab_Notebooks/out/']\n",
    "for i, [imgs, target] in  enumerate(data_loader): #, start=1\n",
    "    with torch.no_grad():\n",
    "        prediction = model([imgs[0].to(device)])[0]\n",
    "    if(i>100):  #150 is number of imgs\n",
    "      break\n",
    "    img_name = output_path + 'img_' + str(i)+'.png' # out/img_0.png\n",
    "    plot(imgs[0], prediction['boxes'], img_name)\n",
    "    #break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "image_folder = 'gdrive/MyDrive/Colab_Notebooks/out' #@param ['gdrive/MyDrive/Colab_Notebooks/out']\n",
    "video_name = 'gdrive/MyDrive/Colab_Notebooks/output_video.avi'\n",
    "\n",
    "images = [img for img in os.listdir(image_folder) if img.endswith(\".png\")]\n",
    "frame = cv2.imread(os.path.join(image_folder, images[0]))\n",
    "height, width, layers = frame.shape\n",
    "\n",
    "video = cv2.VideoWriter(video_name, 0, 10, (width,height)) # cv2.VideoWriter_fourcc(*'XVID'), 0, 1\n",
    "\n",
    "for image in images:\n",
    "    video.write(cv2.imread(os.path.join(image_folder, image)))\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying to open image\n",
    "from PIL import Image\n",
    "import os\n",
    "Image.open('gdrive/MyDrive/Colab_Notebooks/faster_rcnn_fpn_training_mot_17/train/MOT17-02/img1/000001.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
